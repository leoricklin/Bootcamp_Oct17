{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_lens():\n",
    "    age_desc = {\n",
    "        1: \"Under 18\", 18: \"18-24\", 25: \"25-34\", 35: \"35-44\", 45: \"45-49\", 50: \"50-55\", 56: \"56+\"\n",
    "    }\n",
    "    occupation_desc = { \n",
    "        0: \"other or not specified\", 1: \"academic/educator\", 2: \"artist\", 3: \"clerical/admin\",\n",
    "        4: \"college/grad student\", 5: \"customer service\", 6: \"doctor/health care\",\n",
    "        7: \"executive/managerial\", 8: \"farmer\", 9: \"homemaker\", 10: \"K-12 student\", 11: \"lawyer\",\n",
    "        12: \"programmer\", 13: \"retired\", 14: \"sales/marketing\", 15: \"scientist\", 16: \"self-employed\",\n",
    "        17: \"technician/engineer\", 18: \"tradesman/craftsman\", 19: \"unemployed\", 20: \"writer\"\n",
    "    }\n",
    "    rating_data = pd.read_csv(\n",
    "        \"ml-1m/ratings.dat\",\n",
    "        sep=\"::\",\n",
    "        engine=\"python\",\n",
    "        encoding=\"latin-1\",\n",
    "        names=['userid', 'movieid', 'rating', 'timestamp'])\n",
    "    user_data = pd.read_csv(\n",
    "        \"ml-1m/users.dat\", \n",
    "        sep='::', \n",
    "        engine='python', \n",
    "        encoding='latin-1',\n",
    "        names=['userid', 'gender', 'age', 'occupation', 'zipcode']\n",
    "    )\n",
    "    user_data['age_desc'] = user_data['age'].apply(lambda x: age_desc[x])\n",
    "    user_data['occ_desc'] = user_data['occupation'].apply(lambda x: occupation_desc[x])\n",
    "    movie_data = pd.read_csv(\n",
    "        \"ml-1m/movies.dat\",\n",
    "        sep='::', \n",
    "        engine='python', \n",
    "        encoding='latin-1',\n",
    "        names=['movieid', 'title', 'genre']\n",
    "    )\n",
    "    dataset = pd.merge(pd.merge(rating_data, movie_data, how=\"left\", on=\"movieid\"), user_data, how=\"left\", on=\"userid\")\n",
    "    adj_col = dataset['movieid']\n",
    "    adj_col_uni = adj_col.sort_values().unique()\n",
    "    adj_df = pd.DataFrame(adj_col_uni).reset_index().rename(columns = {0:'movieid','index':'adj_movieid'})\n",
    "    dataset = pd.merge(adj_df,dataset,how=\"right\", on=\"movieid\")\n",
    "    dataset['adj_userid'] = dataset['userid'] - 1\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset, split_frac=.7):\n",
    "    dataset = dataset.sample(frac=1, replace=False)\n",
    "    n_split = int(len(dataset)*split_frac)\n",
    "    trainset = dataset[:n_split]\n",
    "    validset = dataset[n_split:]\n",
    "    return trainset, validset\n",
    "\n",
    "fullset = load_movie_lens()\n",
    "trainset, validset = split_dataset(fullset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAT_STR_COLS = [\"genre\", \"zipcode\", \"gender\"]\n",
    "CAT_INT_COLS = [ \"age\", \"occupation\"]\n",
    "LABEL_COL = \"rating\"\n",
    "DEEP_COLS = CAT_STR_COLS + CAT_INT_COLS\n",
    "WIDE_COL_CROSSES = [[\"age\", \"genre\"],[\"gender\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(dataframe):\n",
    "    feature_inputs = {\n",
    "        col_name: tf.SparseTensor(\n",
    "            indices = [[i, 0] for i in range(len(dataframe[col_name]))],\n",
    "            values = dataframe[col_name].values,\n",
    "            dense_shape = [len(dataframe[col_name]), 1]\n",
    "        )\n",
    "        for col_name in CAT_STR_COLS + CAT_INT_COLS\n",
    "    }\n",
    "    label_input = tf.constant(dataframe[LABEL_COL].values-1)\n",
    "    return (feature_inputs, label_input)\n",
    "\n",
    "def make_hash_layers():\n",
    "    hashed_layers = {\n",
    "        col_name : tf.feature_column.categorical_column_with_hash_bucket(col_name, hash_bucket_size=1000) \n",
    "        for col_name in CAT_STR_COLS\n",
    "    }\n",
    "    return hashed_layers\n",
    "\n",
    "def make_int_layers():\n",
    "    age = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"age\", [1,18,25,35,45, 50, 56])\n",
    "    occupation = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"occupation\", [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "    age = tf.feature_column.indicator_column(age)\n",
    "    occupation = tf.feature_column.indicator_column(occupation)\n",
    "    int_layers = [age, occupation]\n",
    "    return int_layers\n",
    "\n",
    "def make_embedding_layers(hashed_layers, dim=6):\n",
    "    embedding_layers = [\n",
    "        tf.feature_column.embedding_column(\n",
    "            hashed_layers[col_name],\n",
    "            dimension=dim\n",
    "        )\n",
    "        for col_name in CAT_STR_COLS\n",
    "    ]\n",
    "    return embedding_layers\n",
    "\n",
    "def make_deep_layers(embedding_layers,int_layers):\n",
    "    return embedding_layers+int_layers\n",
    "\n",
    "def make_wide_input_layers():\n",
    "    crossed_wide_input_layers = [\n",
    "        tf.feature_column.crossed_column([c for c in cs], hash_bucket_size=int(10**(3+len(cs))))\n",
    "        for cs in WIDE_COL_CROSSES\n",
    "    ]\n",
    "    return crossed_wide_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create input layers...done!\n",
      "create model...INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x122268b38>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './model/'}\n",
      "done!\n",
      "training model...WARNING:tensorflow:From /Users/jl186130/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.67744, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.975172\n",
      "INFO:tensorflow:loss = 1.45055, step = 101 (102.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.01676\n",
      "INFO:tensorflow:loss = 1.44516, step = 201 (98.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.01795\n",
      "INFO:tensorflow:loss = 1.44153, step = 301 (98.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.04164\n",
      "INFO:tensorflow:loss = 1.43867, step = 401 (96.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.03333\n",
      "INFO:tensorflow:loss = 1.43628, step = 501 (96.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.996659\n",
      "INFO:tensorflow:loss = 1.43419, step = 601 (100.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 605 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.892119\n",
      "INFO:tensorflow:loss = 1.4324, step = 701 (112.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.97808\n",
      "INFO:tensorflow:loss = 1.43083, step = 801 (102.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.999122\n",
      "INFO:tensorflow:loss = 1.42942, step = 901 (100.087 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.42816.\n",
      "done!\n",
      "evaluating model...WARNING:tensorflow:From /Users/jl186130/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-29-10:23:32\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-29-10:23:37\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.353139, global_step = 1000, loss = 1.42855\n",
      "done!\n",
      "calculating predictions...INFO:tensorflow:Restoring parameters from ./model/model.ckpt-1000\n",
      "done!\n",
      "calculating probabilites...INFO:tensorflow:Restoring parameters from ./model/model.ckpt-1000\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"create input layers...\", end=\"\")\n",
    "#input_layers = make_input_layers()\n",
    "hashed_layers =make_hash_layers()\n",
    "int_layers = make_int_layers()\n",
    "embedding_layers = make_embedding_layers(hashed_layers,dim =6)\n",
    "deep_input_layers = make_deep_layers(embedding_layers,int_layers)\n",
    "wide_input_layers = make_wide_input_layers()\n",
    "print(\"done!\")\n",
    "print(\"create model...\", end=\"\")\n",
    "model = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    n_classes=5,\n",
    "    linear_feature_columns = wide_input_layers,\n",
    "    dnn_feature_columns = deep_input_layers,\n",
    "    dnn_hidden_units = [32, 16],\n",
    "    fix_global_step_increment_bug=True,\n",
    "    config = tf.contrib.learn.RunConfig(\n",
    "        keep_checkpoint_max = 1,\n",
    "        save_summary_steps = 10,\n",
    "        model_dir = \"./model/\"\n",
    "    )\n",
    ")\n",
    "print(\"done!\")\n",
    "print(\"training model...\", end=\"\")\n",
    "model.fit(input_fn = lambda: make_inputs(trainset), steps=1000)\n",
    "print(\"done!\")\n",
    "print(\"evaluating model...\", end=\"\")\n",
    "results = model.evaluate(input_fn = lambda: make_inputs(validset), steps=1)\n",
    "print(\"done!\")\n",
    "print(\"calculating predictions...\", end=\"\")\n",
    "predictions = model.predict_classes(input_fn = lambda: make_inputs(validset))\n",
    "print(\"done!\")\n",
    "print(\"calculating probabilites...\", end=\"\")\n",
    "probabilities = model.predict_proba(input_fn = lambda: make_inputs(validset))\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4285545\n",
      "accuracy: 0.35313916\n",
      "global_step: 1000\n"
     ]
    }
   ],
   "source": [
    "for n, r in results.items():\n",
    "    print(\"%s: %a\"%(n, r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
